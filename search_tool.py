import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Any
from dataclasses import dataclass
import time
import re
from urllib.parse import quote

@dataclass
class SearchResult:
    title: str
    url: str
    snippet: str
    content: str = ""
    date_published: str = ""
    authors: List[str] = None
    categories: List[str] = None
    paper_id: str = ""

class ArxivSearchTool:
    def __init__(self):
        self.search_history = []
        self.searched_queries = set()
        self.base_url = "http://export.arxiv.org/api/query"
    
    def generate_search_queries(self, question: str, max_queries: int = 5) -> List[str]:
        """
        æ ¹æ®é—®é¢˜ç”ŸæˆarXivæœç´¢æŸ¥è¯¢
        """
        queries = []
        
        # æ¸…ç†é—®é¢˜ï¼Œç§»é™¤ä¸­æ–‡åœç”¨è¯
        stop_words = ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'æ€ä¹ˆ', 'è°', 'ä»€ä¹ˆæ—¶å€™', 'åœ¨å“ªé‡Œ', 'çš„', 'æ˜¯', 'åœ¨', 'æœ‰']
        question_cleaned = question
        for word in stop_words:
            question_cleaned = question_cleaned.replace(word, '')
        
        # ç”Ÿæˆä¸»æŸ¥è¯¢
        main_query = question_cleaned.strip()
        if main_query and main_query not in self.searched_queries:
            queries.append(main_query)
        
        # å¦‚æœæ˜¯ä¸­æ–‡æŸ¥è¯¢ï¼Œä¹Ÿå°è¯•è‹±æ–‡å…³é”®è¯
        if re.search(r'[\u4e00-\u9fff]', question):
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸­è‹±æ–‡æœ¯è¯­å¯¹ç…§è¡¨
            english_terms = self._translate_to_english_terms(question)
            for term in english_terms:
                if term not in self.searched_queries and len(queries) < max_queries:
                    queries.append(term)
        
        # ç”Ÿæˆæ›´å…·ä½“çš„æŸ¥è¯¢
        words = question_cleaned.split()
        if len(words) >= 2:
            for i in range(min(3, len(words)-1)):
                sub_query = ' '.join(words[i:i+2])
                if sub_query and sub_query not in self.searched_queries and len(queries) < max_queries:
                    queries.append(sub_query)
        
        return queries[:max_queries]
    
    def _translate_to_english_terms(self, chinese_text: str) -> List[str]:
        """
        å°†ä¸­æ–‡æœ¯è¯­è½¬æ¢ä¸ºè‹±æ–‡æœç´¢è¯
        """
        # å¸¸è§å­¦æœ¯æœ¯è¯­å¯¹ç…§è¡¨
        term_map = {
            'æ·±åº¦å­¦ä¹ ': 'deep learning',
            'æœºå™¨å­¦ä¹ ': 'machine learning', 
            'äººå·¥æ™ºèƒ½': 'artificial intelligence',
            'ç¥ç»ç½‘ç»œ': 'neural network',
            'è‡ªç„¶è¯­è¨€å¤„ç†': 'natural language processing',
            'è®¡ç®—æœºè§†è§‰': 'computer vision',
            'å¼ºåŒ–å­¦ä¹ ': 'reinforcement learning',
            'å¤§è¯­è¨€æ¨¡å‹': 'large language model',
            'å˜æ¢å™¨': 'transformer',
            'æ³¨æ„åŠ›æœºåˆ¶': 'attention mechanism',
            'å·ç§¯ç¥ç»ç½‘ç»œ': 'convolutional neural network',
            'ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ': 'generative adversarial network',
        }
        
        english_terms = []
        for chinese, english in term_map.items():
            if chinese in chinese_text:
                english_terms.append(english)
        
        return english_terms
    
    def search_papers(self, queries: List[str], max_results: int = 10, 
                     date_from: str = None, categories: List[str] = None) -> List[SearchResult]:
        """
        æœç´¢arXivè®ºæ–‡
        """
        all_results = []
        
        for query in queries:
            if query in self.searched_queries:
                continue
                
            self.searched_queries.add(query)
            print(f"ğŸ” æœç´¢arXivè®ºæ–‡: {query}")
            
            try:
                results = self._search_arxiv_api(query, max_results, date_from, categories)
                all_results.extend(results)
                print(f"   æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡")
                
                # é¿å…APIé¢‘ç‡é™åˆ¶
                time.sleep(1)
                
            except Exception as e:
                print(f"   æœç´¢å‡ºé”™: {e}")
                continue
        
        self.search_history.extend(queries)
        return all_results
    
    def _search_arxiv_api(self, query: str, max_results: int, 
                         date_from: str = None, categories: List[str] = None) -> List[SearchResult]:
        """
        è°ƒç”¨arXiv APIæœç´¢è®ºæ–‡
        """
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = f'all:{query}'
        
        # æ·»åŠ ç±»åˆ«è¿‡æ»¤
        if categories:
            cat_query = ' OR '.join([f'cat:{cat}' for cat in categories])
            search_query = f'({search_query}) AND ({cat_query})'
        
        # APIå‚æ•°
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': max_results,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        # å‘é€è¯·æ±‚
        response = requests.get(self.base_url, params=params, timeout=10)
        response.raise_for_status()
        
        # è§£æXMLå“åº”
        return self._parse_arxiv_response(response.text)
    
    def _parse_arxiv_response(self, xml_content: str) -> List[SearchResult]:
        """
        è§£æarXiv APIçš„XMLå“åº”
        """
        results = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # arXiv APIä½¿ç”¨Atomå‘½åç©ºé—´
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entries = root.findall('atom:entry', ns)
            
            for entry in entries:
                # æå–åŸºæœ¬ä¿¡æ¯
                title = entry.find('atom:title', ns)
                title_text = title.text.strip().replace('\n', ' ') if title is not None else ""
                
                summary = entry.find('atom:summary', ns)
                summary_text = summary.text.strip().replace('\n', ' ') if summary is not None else ""
                
                # æå–URL
                id_elem = entry.find('atom:id', ns)
                paper_url = id_elem.text if id_elem is not None else ""
                
                # æå–è®ºæ–‡ID
                paper_id = paper_url.split('/')[-1] if paper_url else ""
                
                # æå–å‘å¸ƒæ—¥æœŸ
                published = entry.find('atom:published', ns)
                pub_date = published.text[:10] if published is not None else ""
                
                # æå–ä½œè€…
                authors = []
                author_elems = entry.findall('atom:author', ns)
                for author in author_elems:
                    name_elem = author.find('atom:name', ns)
                    if name_elem is not None:
                        authors.append(name_elem.text)
                
                # æå–ç±»åˆ«
                categories = []
                category_elems = entry.findall('atom:category', ns)
                for cat in category_elems:
                    term = cat.get('term')
                    if term:
                        categories.append(term)
                
                # åˆ›å»ºæœç´¢ç»“æœ
                result = SearchResult(
                    title=title_text,
                    url=paper_url,
                    snippet=summary_text[:300] + "..." if len(summary_text) > 300 else summary_text,
                    content=summary_text,
                    date_published=pub_date,
                    authors=authors,
                    categories=categories,
                    paper_id=paper_id
                )
                
                results.append(result)
                
        except ET.ParseError as e:
            print(f"XMLè§£æé”™è¯¯: {e}")
        except Exception as e:
            print(f"å“åº”å¤„ç†é”™è¯¯: {e}")
        
        return results
    
    def download_paper(self, paper_id: str) -> Dict[str, Any]:
        """
        ä¸‹è½½è®ºæ–‡è¯¦ç»†ä¿¡æ¯
        """
        try:
            # è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            params = {
                'id_list': paper_id,
                'max_results': 1
            }
            
            response = requests.get(self.base_url, params=params, timeout=10)
            response.raise_for_status()
            
            results = self._parse_arxiv_response(response.text)
            
            if results:
                paper = results[0]
                return {
                    'success': True,
                    'paper': paper,
                    'pdf_url': f"https://arxiv.org/pdf/{paper_id}.pdf"
                }
            else:
                return {'success': False, 'error': 'è®ºæ–‡æœªæ‰¾åˆ°'}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_search_history(self) -> List[str]:
        """è·å–æœç´¢å†å²"""
        return self.search_history.copy()
    
    def clear_history(self):
        """æ¸…é™¤æœç´¢å†å²"""
        self.search_history.clear()
        self.searched_queries.clear()